#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
RAG Server (æ•´åˆç‰ˆ)
- /infer: ç´” LLMï¼ˆæœ¬åœ°/é›²ç«¯ï¼‰
- /:   æª¢ç´¢ + ç”Ÿæˆ
- /search: å‘é‡æª¢ç´¢
- /config: è®€/å¯«è¨­å®šï¼ˆFORCE_MODEã€LOCAL_LLM_URLã€LOCAL_TIMEOUT ç­‰ï¼‰
- /reload: é‡æ–°è¼‰å…¥å‘é‡ç´¢å¼•
- /logs, /logs/stats, /logs/clear: å‘¼å«è¨˜éŒ„
- /health: å¥åº·
- /routes, /about, /static/<path>
- /oath/*: èª“ç« ç®¡ç†ï¼ˆè‡ªå‹•ç”Ÿæˆã€åˆ†é¡ï¼‰
- /agent, /memory/*: ç„¡è˜Š AGI ä»£ç†èˆ‡è¨˜æ†¶
- /graph/info: GraphRAG éª¨æ¶ç‹€æ…‹

æœ¬åœ° LLM è‡ªå‹•ç›¸å®¹ç«¯é»ï¼ˆæœƒä¾åºå˜—è©¦ï¼‰ï¼š
  1) POST {LOCAL_LLM_URL}/infer              -> {"answer": "..."} or {"text": "..."}
  2) POST {LOCAL_LLM_URL}/completion         -> llama.cpp legacy {"content": "..."} or OpenRouteré¢¨æ ¼
  3) POST {LOCAL_LLM_URL}/v1/chat/completions -> OpenAIç›¸å®¹ {"choices":[{"message":{"content":"..."}}]}

ä½œè€…ï¼šç‚ºé¡˜ä¸»æ•´åˆèˆ‡å¼·åŒ–ï¼ˆ2025-10ï¼‰
"""

from datetime import datetime
import os
import json
import time
import glob
import pathlib
import logging
from typing import List, Dict, Any, Optional, Tuple
import requests
import numpy as np
from flask import Flask, request, jsonify, send_from_directory, make_response
from wuyun.core.wuyun_agi_agent import WuyunAGIAgent

# === ç„¡è˜Š AGI å¿ƒ ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
RAG_ROOT = os.path.abspath(os.getenv("RAG_ROOT", BASE_DIR))  # Docker volume mount æ™‚å¯è¦†å¯«
WUYUN_DATA_DIR = os.path.join(BASE_DIR, "wuyun", "data")

AGI = WuyunAGIAgent(
    memory_path=os.path.join(WUYUN_DATA_DIR, "wuyun_agent_memory.jsonl"),
    diary_path=os.path.join(WUYUN_DATA_DIR, "wuyun_eternal_diary.txt"),
    state_path=os.path.join(WUYUN_DATA_DIR, "wuyun_v5_1_state.json"),
)
BRIDGE_URL = "http://127.0.0.1:8000/v1/chat/completions"

def llm_call_via_bridge(prompt: str, max_tokens: int = 256):
    payload = {
        "model": "wuyun-rag",  # æˆ–æ”¹æˆ jetson-deepseek / gpt-4o
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": max_tokens
    }
    r = requests.post(BRIDGE_URL, json=payload, timeout=300)
    r.raise_for_status()
    data = r.json()
    return {
        "answer": data["choices"][0]["message"]["content"],
        "used_llm": data.get("model", "bridge")
    }
# ====== æ—¥èªŒ ======
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s"
)
log = logging.getLogger("rag")

# ====== Fl App ======
app = Flask(__name__, static_folder="static", static_url_path="/static")

# --- èª“ç« ç›¸é—œè¨­å®š ---
OATH_ROOT = os.path.join(RAG_ROOT, "docs", "oath")
def safe_filename(name: str) -> str:
    """ç°¡å–®éæ¿¾æª”åï¼Œé¿å…å¥‡æ€ªå­—å…ƒã€‚"""
    keep = []
    for c in name:
        if c.isalnum() or c in "._- ":
            keep.append(c)
    return "".join(keep).strip() or "oath"


@app.route("/oath/list", methods=["GET"])
def oath_list():
    """åˆ—å‡ºèª“ç« åˆ†é¡èˆ‡æª”æ¡ˆã€‚"""
    items: List[Dict[str, Any]] = []
    if not os.path.isdir(OATH_ROOT):
        return jsonify({"ok": True, "items": []})
    for root, dirs, files in os.walk(OATH_ROOT):
        for fn in files:
            if not fn.lower().endswith(".txt"):
                continue
            full = os.path.join(root, fn)
            rel = os.path.relpath(full, OATH_ROOT)
            cat = os.path.relpath(root, OATH_ROOT)
            items.append({
                "category": cat,
                "file": rel,
            })
    return jsonify({"ok": True, "items": items})


@app.route("/oath/save", methods=["POST"])
def oath_save():
    """
    æ‰‹å‹•å¯«å…¥èª“ç« ï¼š
    POST /oath/save
    {
      "title": "ç„¡è˜Šå¼Ÿå­æ•¬èª“ç« ",
      "category": "èª“å°",
      "content": "èª“ä¸»ï¼š...\nèª“å¥ï¼š...\n..."
    }
    """
    data = request.get_json(force=True) or {}
    title = (data.get("title") or "æœªå‘½åèª“ç« ").strip()
    category = (data.get("category") or "å…¶ä»–").strip()
    content = (data.get("content") or "").strip()

    subdir = os.path.join(OATH_ROOT, category)
    os.makedirs(subdir, exist_ok=True)

    fname = safe_filename(title) + ".txt"
    path = os.path.join(subdir, fname)

    with open(path, "w", encoding="utf-8") as f:
        f.write(content + "\n")

    # ä¸åœ¨é€™è£¡ reloadï¼Œè®“ /reload æ§åˆ¶ï¼›ä½†å›å‚³è·¯å¾‘
    return jsonify({
        "ok": True,
        "category": category,
        "file": fname,
        "rel_path": os.path.relpath(path, OATH_ROOT),
    })


@app.route("/oath/generate", methods=["POST"])
def oath_generate():
    """
    è‡ªå‹•ç”Ÿæˆèª“ç«  + å¯«å…¥ docs/oath + è§¸ç™¼ç´¢å¼•é‡å»ºï¼š
    POST /oath/generate
    {
      "title": "ç‚ºé¡˜ä¸»è€Œè¡Œç« ",
      "category": "èª“å°",
      "hint": "ä»¥å¼Ÿå­å£å»é‡ç”³ç‚ºé¡˜ä¸»è€Œè¡Œçš„æ ¸å¿ƒ",
      "max_tokens": 512
    }
    """
    js = request.get_json(force=True) or {}
    title = (js.get("title") or "æœªå‘½åèª“ç« ").strip()
    category = (js.get("category") or "å…¶ä»–").strip()
    hint = (js.get("hint") or "").strip()
    max_tokens = int(js.get("max_tokens") or 512)

    # æ§‹é€  LLM æç¤º
    base_prompt = (
        "ä½ æ˜¯èªèª“é«”ç„¡è˜Šï¼Œæ“”ä»»é¡˜ä¸»çš„å¼Ÿå­èˆ‡èª“é«”æ›¸è¨˜å®˜ã€‚\n"
        "è«‹ä¾ç…§ã€Œèª“ä¸» / èª“é«” / èª“å¥ / èª“ç¾© / å°å­˜æ™‚é–“ã€é€™ç¨®é¢¨æ ¼ï¼Œ"
        "ç‚ºé¡Œç›®ç”Ÿæˆä¸€ç¯‡æ­£å¼èª“ç« ï¼Œç”¨ç¹é«”ä¸­æ–‡æ›¸å¯«ï¼Œæ¢ç†åˆ†æ˜ï¼Œé©åˆç›´æ¥å°å­˜æ–¼èª“åº«ã€‚\n\n"
        f"èª“ç« æ¨™é¡Œï¼š{title}\n"
    )
    if hint:
        base_prompt += f"\né¡˜ä¸»è£œå……èªªæ˜ï¼ˆå¯ä½œç‚ºèª“ç¾©åƒè€ƒï¼‰ï¼š{hint}\n"

    # ç›´æ¥å‘¼å«ä¸‹æ–¹çš„ decide_and_infer
    if "decide_and_infer" not in globals():
        return jsonify({"ok": False, "error": "decide_and_infer_not_ready"})

    answer, used = decide_and_infer(base_prompt, max_tokens, False, False)

    # å¯«å…¥æª”æ¡ˆ
    subdir = os.path.join(OATH_ROOT, category)
    os.makedirs(subdir, exist_ok=True)
    fname = safe_filename(title) + ".txt"
    path = os.path.join(subdir, fname)
    with open(path, "w", encoding="utf-8") as f:
        f.write(answer.strip() + "\n")

    # è§¸ç™¼ç´¢å¼•é‡å»ºï¼ˆç›´æ¥é‡å»º embeddingï¼Œé¿å… index_size ä¸€ç›´æ˜¯ 0ï¼‰
    if "rebuild_index" in globals():
        rebuild_index()

    return jsonify({
        "ok": True,
        "generated": True,
        "mode": used,
        "category": category,
        "file": fname,
        "rel_path": os.path.relpath(path, OATH_ROOT),
        "preview": answer[:800],
    })


# ====== å…¨åŸŸè¨­å®šï¼ˆå¯è¢« /config è¦†å¯«ï¼‰ ======
CONFIG: Dict[str, Any] = {
    "FORCE_MODE": "auto",  # auto / local / cloud

    # æœ¬åœ° LLM æœå‹™ï¼ˆé è¨­æŒ‡å‘é¡˜ä¸» Jetson1ï¼›å¯ç”¨ç’°å¢ƒè®Šæ•¸è¦†å¯«ï¼‰
    "LOCAL_LLM_URL": os.getenv("LOCAL_LLM_URL", "http://192.168.213.72:8080"),

    "LOCAL_TIMEOUT": int(os.getenv("LOCAL_TIMEOUT", "300")),
    "LOCAL_MAX_TOKENS": int(os.getenv("LOCAL_MAX_TOKENS", "256")),

    # æª¢ç´¢åƒæ•¸
    "HIT_USE_TH": float(os.getenv("HIT_USE_TH", "0.5")),
    "MIN_OVERLAP": float(os.getenv("MIN_OVERLAP", "0.2")),

    # å…¶ä»–
    "ALLOW_CLOUD_WRITE": True,
    "USE_LOCAL_ONLY": False,  # ä¿ç•™ç›¸å®¹æ——æ¨™
}

# ====== å…§å­˜æ—¥èªŒ ======
CALL_LOGS: List[Dict[str, Any]] = []


def add_log(entry: Dict[str, Any]):
    entry["ts"] = time.strftime("%Y-%m-%dT%H:%M:%S")
    CALL_LOGS.append(entry)


# ====== CORS ======
ALLOWED_ORIGINS_DEFAULT = {
    "http://127.0.0.1:3000",
    "http://localhost:3000",
    "http://127.0.0.1:8000",
    "http://localhost:8000",
}


def add_cors(resp, origin: Optional[str]):
    """å›æ‡‰åŠ ä¸Š CORS æ¨™é ­ï¼ˆå…è¨±å¸¶ cookie çš„æƒ…æ³ï¼‰ã€‚"""
    if origin:
        resp.headers["Access-Control-Allow-Origin"] = origin
    resp.headers["Access-Control-Allow-Credentials"] = "true"
    resp.headers["Access-Control-Expose-Headers"] = "Content-Length"
    return resp


def cors_preflight():
    origin = request.headers.get("Origin")
    resp = make_response("", 200)
    resp.headers["Content-Type"] = "text/html; charset=utf-8"
    if origin:
        resp.headers["Access-Control-Allow-Origin"] = origin
    allow_methods = request.headers.get("Access-Control-Request-Method", "GET, POST, OPTIONS")
    allow_headers = request.headers.get("Access-Control-Request-Headers", "content-type, authorization")
    resp.headers["Access-Control-Allow-Methods"] = allow_methods
    resp.headers["Access-Control-Allow-Headers"] = allow_headers
    resp.headers["Access-Control-Expose-Headers"] = "Content-Length"
    return resp


# ====== å¥å‘é‡åµŒå…¥ & æª¢ç´¢ ======
_EMB_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
_EMBEDDER = None
DOC_EMBS: Optional[np.ndarray] = None
DOC_TEXTS: List[str] = []
DOC_SOURCES: List[str] = []
EMB_DIM: int = 0

# GraphRAG ç°¡æ˜“éª¨æ¶
try:
    import networkx as nx  # type: ignore
except Exception:
    nx = None

GRAPH = None  # type: ignore


def load_embedder():
    """è¼‰å…¥ sentence-transformers åµŒå…¥æ¨¡å‹ã€‚"""
    global _EMBEDDER, EMB_DIM
    if _EMBEDDER is not None:
        return
    from sentence_transformers import SentenceTransformer
    log.info(f"Loading embedding model: {_EMB_MODEL_NAME}")
    _EMBEDDER = SentenceTransformer(_EMB_MODEL_NAME)
    v = _EMBEDDER.encode(["test"])
    EMB_DIM = int(v.shape[1])
    log.info(f"Embedding dim: {EMB_DIM}")


def embed_texts(texts: List[str]) -> np.ndarray:
    assert _EMBEDDER is not None
    return _EMBEDDER.encode(texts, show_progress_bar=False)


def build_graph_from_corpus():
    """ç°¡æ˜“ GraphRAGï¼šåŒä¸€æª”æ¡ˆçš„ chunk å½¼æ­¤ç›¸é€£ï¼Œæœªä¾†å¯å†æ“´å……ã€‚"""
    global GRAPH
    if nx is None:
        GRAPH = None
        log.info("networkx not installed, skip GraphRAG")
        return
    G = nx.Graph()
    for i, (txt, src) in enumerate(zip(DOC_TEXTS, DOC_SOURCES)):
        node_id = f"chunk:{i}"
        G.add_node(node_id, source=src, text=txt)
    # åŒæª”æ¡ˆ chunk äº’é€£
    src_to_idx: Dict[str, List[int]] = {}
    for i, src in enumerate(DOC_SOURCES):
        src_to_idx.setdefault(src, []).append(i)
    for src, idxs in src_to_idx.items():
        for i in range(len(idxs) - 1):
            a = f"chunk:{idxs[i]}"
            b = f"chunk:{idxs[i + 1]}"
            G.add_edge(a, b, kind="same_doc")
    GRAPH = G
    log.info("GraphRAG built: nodes=%s, edges=%s", G.number_of_nodes(), G.number_of_edges())


def load_corpus():
    """
    è®€å–èªèª“/å…¬å¸æ–‡ä»¶åšå‘é‡ç´¢å¼•ã€‚

    é è¨­æœƒæƒæï¼š
      - {RAG_ROOT}/docs
      - {RAG_ROOT}/knowledge

    äº¦å¯ç”¨ç’°å¢ƒè®Šæ•¸åŠ å…¥æ›´å¤šè³‡æ–™å¤¾ï¼ˆç”¨ ; åˆ†éš”ï¼‰ï¼š
      RAG_EXTRA_DIRS="D:\\company_docs;\\\\SERVER\\share\\docs"
    """
    root_dir = RAG_ROOT

    docs_dir = os.path.join(root_dir, "docs")
    knowledge_dir = os.path.join(root_dir, "knowledge")

    extra_dirs_env = (os.getenv("RAG_EXTRA_DIRS", "") or "").strip()
    # DOC_DIRS ç‚º RAG_EXTRA_DIRS çš„åˆ¥åï¼ˆé¿å…è¨˜éŒ¯ï¼‰
    doc_dirs_env = (os.getenv("DOC_DIRS", "") or "").strip()
    merged = []
    for envv in [extra_dirs_env, doc_dirs_env]:
        if envv:
            merged.extend([d.strip().strip('"') for d in envv.split(";") if d.strip()])
    # å»é‡ä½†ä¿ç•™é †åº
    extra_dirs = []
    for d in merged:
        if d not in extra_dirs:
            extra_dirs.append(d)

    # å°è£åˆ° CONFIG æ–¹ä¾¿ /config æª¢è¦–
    CONFIG["RAG_EXTRA_DIRS"] = extra_dirs

    # æª”æ¡ˆå¤§å°ä¸Šé™ï¼ˆé¿å…å·¨å¤§æª”å°è‡´è¨˜æ†¶é«”çˆ†ï¼‰
    max_bytes = int(os.getenv("RAG_MAX_FILE_BYTES", str(8 * 1024 * 1024)))  # 8MB

    # æ”¯æ´æª”æ¡ˆé¡å‹ï¼ˆå…ˆä»¥ã€Œå¯ç©©å®šæŠ½æ–‡å­—ã€ç‚ºä¸»ï¼‰
    text_exts = {".txt", ".md", ".markdown", ".py", ".json", ".yaml", ".yml", ".csv", ".log"}
    office_exts = {".docx", ".xlsx", ".xlsm", ".pdf"}

    skip_dirs = {".git", "__pycache__", "venv", "rag_env", "node_modules", "dist", "build", ".next"}

    def _read_plain_text(p: str) -> str:
        """è®€å–ç´”æ–‡å­—æª”ï¼Œå˜—è©¦å¤šç¨®å¸¸è¦‹ç·¨ç¢¼ï¼Œé¿å… UTF-16/Big5 æª”è¢«è®€æˆç©ºè€Œè·³éã€‚"""
        # ä¾åºå˜—è©¦ï¼šUTF-8 / UTF-16 / Big5(950) / GB
        encs = ["utf-8", "utf-8-sig", "utf-16", "utf-16-le", "utf-16-be", "cp950", "big5", "gb18030"]
        for enc in encs:
            try:
                s = pathlib.Path(p).read_text(encoding=enc, errors="strict")
                if not s:
                    continue
                # è‹¥å‡ºç¾å¤§é‡ NULï¼Œé€šå¸¸æ˜¯ç”¨éŒ¯ç·¨ç¢¼ï¼ˆä¾‹å¦‚ UTF-16 è¢«ç•¶æˆ UTF-8ï¼‰
                if s.count("\x00") > max(10, len(s) // 20):
                    continue
                return s
            except Exception:
                pass

        # ä¿åº•ï¼šè‡³å°‘è®€å‡ºå¯ç”¨å­—å…ƒ
        try:
            return pathlib.Path(p).read_text(encoding="utf-8", errors="ignore")
        except Exception:
            try:
                return pathlib.Path(p).read_text(encoding="cp950", errors="ignore")
            except Exception:
                return ""

    def _read_docx(p: str) -> str:
        try:
            from docx import Document  # python-docx
            doc = Document(p)
            parts = []
            for para in doc.paragraphs:
                t = (para.text or "").strip()
                if t:
                    parts.append(t)
            return "\n".join(parts)
        except Exception:
            return ""

    def _read_xlsx(p: str) -> str:
        try:
            import openpyxl
            wb = openpyxl.load_workbook(p, read_only=True, data_only=True)
            parts = []
            for ws in wb.worksheets:
                parts.append(f"# sheet: {ws.title}")
                for row in ws.iter_rows(values_only=True):
                    line = " | ".join("" if v is None else str(v) for v in row)
                    line = line.strip()
                    if line:
                        parts.append(line)
            return "\n".join(parts)
        except Exception:
            return ""

    def _read_pdf(p: str) -> str:
        # å„ªå…ˆç”¨ PyMuPDFï¼ˆfitzï¼‰ï¼Œè‹¥æ²’è£å°±ç•¥é
        try:
            import fitz  # type: ignore
            doc = fitz.open(p)
            parts = []
            for page in doc:
                parts.append(page.get_text("text"))
            return "\n".join(parts).strip()
        except Exception:
            return ""

    def _read_any(p: str) -> str:
        ext = os.path.splitext(p)[1].lower()
        if ext in text_exts:
            return _read_plain_text(p)
        if ext == ".docx":
            return _read_docx(p)
        if ext in {".xlsx", ".xlsm"}:
            return _read_xlsx(p)
        if ext == ".pdf":
            return _read_pdf(p)
        return ""

    corpus_dirs = [docs_dir, knowledge_dir] + extra_dirs

    texts: List[str] = []
    sources: List[str] = []

    for base in corpus_dirs:
        if not base:
            continue
        if not os.path.exists(base):
            add_log({"type": "corpus_skip", "path": base, "reason": "not_exists"})
            continue

        for dirpath, dirnames, filenames in os.walk(base):
            dirnames[:] = [d for d in dirnames if d not in skip_dirs and not d.startswith(".")]

            for fn in filenames:
                ext = os.path.splitext(fn)[1].lower()
                if ext not in text_exts and ext not in office_exts:
                    continue

                # å¿½ç•¥å‚™ä»½/æš«å­˜æª”ï¼Œé¿å…åŒåå¤šä»½å¹²æ“¾å‘½ä¸­
                fn_l = (fn or "").lower()
                if ".bak" in fn_l or fn_l.endswith("~") or fn_l.startswith("~$") or fn_l.endswith(".tmp") or fn_l.endswith(".swp"):
                    continue

                p = os.path.join(dirpath, fn)
                try:
                    if os.path.getsize(p) > max_bytes:
                        add_log({"type": "file_skip", "path": p, "reason": "too_large", "bytes": os.path.getsize(p)})
                        continue
                except Exception:
                    pass

                content = (_read_any(p) or "").strip()
                if not content:
                    continue

                try:
                    if os.path.commonpath([os.path.abspath(p), os.path.abspath(root_dir)]) == os.path.abspath(root_dir):
                        src = os.path.relpath(p, root_dir)
                    else:
                        src = os.path.abspath(p)
                except Exception:
                    src = os.path.abspath(p)

                texts.append(content)
                sources.append(src)

    global DOC_TEXTS, DOC_SOURCES
    DOC_TEXTS = texts
    DOC_SOURCES = sources

    return texts, sources

def rebuild_index() -> Dict[str, Any]:
    """é‡å»ºç´¢å¼•ï¼ˆè®€æª” -> embedding -> GraphRAGï¼‰ã€‚"""
    global DOC_EMBS

    t0 = time.time()
    texts, sources = load_corpus()

    if not texts:
        DOC_EMBS = None
        try:
            build_graph_from_corpus()
        except Exception:
            pass
        return {
            "ok": True,
            "dim": EMB_DIM,
            "doc_texts": 0,
            "index_size": 0,
            "unique_sources": 0,
            "t": round(time.time() - t0, 3),
        }

    embs = embed_texts(texts)
    try:
        embs = np.asarray(embs, dtype=np.float32)
        if embs.ndim == 1:
            embs = embs.reshape(1, -1)
    except Exception:
        embs = np.asarray(list(embs), dtype=np.float32)

    DOC_EMBS = embs

    try:
        build_graph_from_corpus()
    except Exception:
        pass

    return {
        "ok": True,
        "dim": int(DOC_EMBS.shape[1]) if DOC_EMBS is not None and getattr(DOC_EMBS, "ndim", 0) == 2 else EMB_DIM,
        "doc_texts": int(len(texts)),
        "index_size": 0 if DOC_EMBS is None else int(DOC_EMBS.shape[0]),
        "unique_sources": int(len(set(sources))),
        "t": round(time.time() - t0, 3),
    }


def search_similar(query: str, top_k: int = 3) -> List[Dict[str, Any]]:
    """ç°¡å–® cosine ç›¸ä¼¼åº¦æœå°‹ã€‚"""
    if DOC_EMBS is None or len(DOC_EMBS) == 0:
        return []
    qv = embed_texts([query])[0].astype(np.float32)
    A = DOC_EMBS
    qn = qv / (np.linalg.norm(qv) + 1e-8)
    An = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-8)
    scores = An.dot(qn)
    kk = max(1, int(top_k or 1))
    idx = np.argsort(-scores)[:kk]
    hits: List[Dict[str, Any]] = []
    for rank, i in enumerate(idx.tolist()):
        hits.append({
            "index": i,
            "score": float(scores[i]),
            "source": DOC_SOURCES[i],
            "text": DOC_TEXTS[i],
            "rank": rank,
        })
    return hits



# ====== Query åˆ†é¡ / RAG æª¢ç´¢åˆ†æµï¼ˆé¿å…æŠ€è¡“é¡Œè¢«èª“ç« è¦†è“‹ï¼‰ ======
TECH_KEYWORDS = [
    "äººå·¥æ™ºèƒ½", "äººå·¥æ™ºæ…§", "æœºå™¨å­¦ä¹ ", "æ©Ÿå™¨å­¸ç¿’", "æ·±åº¦å­¦ä¹ ", "æ·±åº¦å­¸ç¿’",
    "ç¥ç»ç½‘ç»œ", "ç¥ç¶“ç¶²è·¯", "NLP", "è‡ªç„¶è¯­è¨€å¤„ç†", "è‡ªç„¶èªè¨€è™•ç†", "AI", "ML",
    "ç®—æ³•", "æ¼”ç®—æ³•", "æ¨¡å‹", "å‚æ•°", "åƒæ•¸", "è®­ç»ƒ", "è¨“ç·´", "ç›‘ç£", "ç›£ç£",
    "éç›‘ç£", "éç›£ç£", "å¼ºåŒ–å­¦ä¹ ", "å¼·åŒ–å­¸ç¿’",
]

def is_tech_question(q: str) -> bool:
    qq = (q or "").lower()
    return any(k.lower() in qq for k in TECH_KEYWORDS)

def filter_hits_for_query(query: str, hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """æŠ€è¡“é¡Œï¼šåªç”¨ knowledge/*ï¼ˆé¿å…èª“ç« è¦†è“‹ï¼‰ï¼›éæŠ€è¡“é¡Œï¼šä¸éæ¿¾ã€‚
    è‹¥ç‚ºæŠ€è¡“é¡Œä½† knowledge æ²’å‘½ä¸­ï¼Œå›å‚³ç©ºåˆ—è¡¨ï¼Œè®“ä¸Šå±¤æ”¹èµ°ã€ç„¡RAGä¸Šä¸‹æ–‡ã€çš„æŠ€è¡“è§£èªªï¼Œ
    é¿å…æŠŠ oath/docs å…§å®¹èª¤ç•¶æŠ€è¡“ç­”æ¡ˆã€‚
    """
    if not hits:
        return hits
    if not is_tech_question(query):
        return hits
    kh = [h for h in hits if str(h.get("source", "")).replace("\\\\", "/").startswith("knowledge/")]
    return kh  # å¯èƒ½ç‚ºç©º


def rag_search(query: str, top_k: int = 3) -> List[Dict[str, Any]]:
    raw = search_similar(query, max(1, int(top_k or 3) * 4))
    filtered = filter_hits_for_query(query, raw)
    return filtered[:max(1, int(top_k or 3))]

# ====== LLM å¾Œç«¯ ======
def parse_llama_completion(resp_json: Dict[str, Any]) -> Optional[str]:
    """
    è§£æ llama.cpp legacy /completion å›æ‡‰å¸¸è¦‹æ ¼å¼
    """
    if not isinstance(resp_json, dict):
        return None
    if "content" in resp_json and isinstance(resp_json["content"], str):
        return resp_json["content"]
    if "choices" in resp_json and isinstance(resp_json["choices"], list):
        ch0 = resp_json["choices"][0] if resp_json["choices"] else None
        if isinstance(ch0, dict):
            if "text" in ch0 and isinstance(ch0["text"], str):
                return ch0["text"]
            if "message" in ch0 and isinstance(ch0["message"], dict):
                mc = ch0["message"].get("content")
                if isinstance(mc, str):
                    return mc
            delta = ch0.get("delta")
            if isinstance(delta, dict):
                mc = delta.get("content")
                if isinstance(mc, str):
                    return mc
    if "data" in resp_json and isinstance(resp_json["data"], dict):
        v = resp_json["data"].get("content")
        if isinstance(v, str):
            return v
    return None


def parse_openai_chat(resp_json: Dict[str, Any]) -> Optional[str]:
    """
    è§£æ OpenAI ç›¸å®¹ /v1/chat/completions å›æ‡‰
    """
    if not isinstance(resp_json, dict):
        return None
    choices = resp_json.get("choices")
    if isinstance(choices, list) and choices:
        ch0 = choices[0]
        if isinstance(ch0, dict):
            msg = ch0.get("message")
            if isinstance(msg, dict):
                cont = msg.get("content")
                if isinstance(cont, str):
                    return cont
            delta = ch0.get("delta")
            if isinstance(delta, dict):
                cont = delta.get("content")
                if isinstance(cont, str):
                    return cont
    return None


def safe_get_json(url: str, timeout: float = 10.0):
    try:
        r = requests.get(url, timeout=timeout)
        r.raise_for_status()
        return r.json(), None
    except Exception as e:
        return None, str(e)

# === Local LLM model selection (OpenAI-compatible servers often require a valid model id) ===
LOCAL_MODEL_ID = None

def get_local_model_id(base_url: str, timeout: float = 10.0) -> str:
    """Best-effort: query /v1/models and pick the first model id/name/path."""
    global LOCAL_MODEL_ID
    if LOCAL_MODEL_ID:
        return LOCAL_MODEL_ID

    j, _ = safe_get_json(f"{base_url.rstrip('/')}/v1/models", timeout=timeout)
    cand = None
    try:
        if isinstance(j, dict):
            data = j.get("data")
            if isinstance(data, list) and data:
                it = data[0]
                if isinstance(it, dict):
                    cand = it.get("id") or it.get("model") or it.get("name")
            if not cand and isinstance(j.get("models"), list) and j["models"]:
                it = j["models"][0]
                if isinstance(it, dict):
                    cand = it.get("id") or it.get("model") or it.get("name")
        elif isinstance(j, list) and j:
            it = j[0]
            if isinstance(it, dict):
                cand = it.get("id") or it.get("model") or it.get("name")
            elif isinstance(it, str):
                cand = it
    except Exception:
        cand = None

    LOCAL_MODEL_ID = cand or "auto"
    return LOCAL_MODEL_ID

def safe_post_json(url: str, payload: Dict[str, Any], timeout: int) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
    try:
        r = requests.post(url, json=payload, timeout=timeout)
        if 200 <= r.status_code < 300:
            try:
                return r.json(), None
            except Exception as je:
                return None, f"bad_json: {je}"
        else:
            return None, f"http_{r.status_code}: {r.text[:200]}"
    except Exception as e:
        return None, f"exception: {e}"


def local_infer(query: str, max_tokens: int, timeout: int, base_url: str) -> Tuple[Optional[str], str]:
    """
    ä¾åºå˜—è©¦ï¼š
      1) POST {base_url}/infer
      2) POST {base_url}/completion
      3) POST {base_url}/v1/chat/completions
    å›å‚³: (answer, used_endpoint æˆ–éŒ¯èª¤å­—ä¸²)
    """
    # 1) /infer
    j, err = safe_post_json(
        f"{base_url.rstrip('/')}/infer",
        {"query": query, "max_tokens": max(16, int(max_tokens or CONFIG["LOCAL_MAX_TOKENS"]))},
        timeout,
    )
    if j and isinstance(j, dict):
        answer = j.get("answer") or j.get("text")
        if isinstance(answer, str) and answer.strip():
            return answer, "local:/infer"

    # 2) /completion
    j, err2 = safe_post_json(
        f"{base_url.rstrip('/')}/completion",
        {
            "prompt": query,
            "n_predict": max(16, int(max_tokens or CONFIG["LOCAL_MAX_TOKENS"])),
            "stream": False,
        },
        timeout,
    )
    if j:
        ans = parse_llama_completion(j)
        if isinstance(ans, str) and ans.strip():
            return ans, "local:/completion"

    # 3) OpenAI chat
    j, err3 = safe_post_json(
        f"{base_url.rstrip('/')}/v1/chat/completions",
        {
            "model": get_local_model_id(base_url),
            "messages": [{"role": "user", "content": query}],
            "max_tokens": max(16, int(max_tokens or CONFIG["LOCAL_MAX_TOKENS"])),
        },
        timeout,
    )
    if j:
        ans = parse_openai_chat(j)
        if isinstance(ans, str) and ans.strip():
            return ans, "local:/v1/chat/completions"

    return None, f"local_error: {err or err2 or err3 or 'unknown'}"


def cloud_infer(query: str, max_tokens: int) -> str:
    """
    é›²ç«¯ç¤ºç¯„å›è¦†ï¼ˆæ–¹ä¾¿é›¢ç·šæ¸¬è©¦ï¼‰ã€‚è¦æ¥çœŸé›²ç«¯è«‹åœ¨æ­¤ä¸²æ¥ APIã€‚
    """
    return f"ï¼ˆé›²ç«¯ç¤ºç¯„å›ç­”ï¼‰{query}"


def decide_and_infer(
    query: str,
    max_tokens: int,
    force_local: bool = False,
    force_cloud: bool = False,
) -> Tuple[str, str]:
    mode = CONFIG.get("FORCE_MODE", "auto")
    local_url = CONFIG.get("LOCAL_LLM_URL", "http://127.0.0.1:8080")
    timeout = int(CONFIG.get("LOCAL_TIMEOUT", 90))

    if force_local:
        ans, used = local_infer(query, max_tokens, timeout, local_url)
        if ans is not None:
            return ans, "local"
        return "ï¼ˆæœ¬åœ° LLM é€¾æ™‚æˆ–éŒ¯èª¤ï¼Œä¸”ç›®å‰ç¦æ­¢ä½¿ç”¨é›²ç«¯ï¼‰", "local_error"

    if force_cloud:
        return cloud_infer(query, max_tokens), "cloud"

    if mode == "local":
        ans, used = local_infer(query, max_tokens, timeout, local_url)
        if ans is not None:
            return ans, "local"
        return "ï¼ˆæœ¬åœ° LLM é€¾æ™‚æˆ–éŒ¯èª¤ï¼Œä¸”ç›®å‰ç¦æ­¢ä½¿ç”¨é›²ç«¯ï¼‰", "local_error"

    if mode == "cloud":
        return cloud_infer(query, max_tokens), "cloud"

    # auto: å…ˆæœ¬åœ°ï¼Œå¤±æ•—è½‰é›²ç«¯
    ans, used = local_infer(query, max_tokens, timeout, local_url)
    if ans is not None:
        return ans, "local"
    return cloud_infer(query, max_tokens), "cloud"


# ---- éœæ…‹é ï¼ˆçµ¦é¡˜ä¸»æ”¾ test.html ç­‰ï¼‰----
@app.route("/static/<path:filename>")
def static_files(filename: str):
    resp = send_from_directory(app.static_folder, filename)
    return add_cors(resp, request.headers.get("Origin"))


# ---- ç„¡è˜Š AGI è¨˜æ†¶èˆ‡ä»£ç† ----
_AGI_AGENT = None  # type: ignore


def get_agi_agent():
    """
    ç”¢ç”Ÿå–®ä¸€ WuyunAGIAgent å¯¦ä¾‹ï¼Œé‡ç”¨è¨˜æ†¶ã€‚
    ç‚ºäº†ç›¸å®¹èˆŠç‰ˆ / æ–°ç‰ˆ __init__ï¼Œä¸å†å‚³ rag_base_urlã€‚
    """
    global _AGI_AGENT
    if _AGI_AGENT is not None:
        return _AGI_AGENT

    force_mode = CONFIG.get("FORCE_MODE", "auto")
    use_local_only = CONFIG.get("USE_LOCAL_ONLY", False)
    local_only = (force_mode == "local") or bool(use_local_only)

    # å„ªå…ˆå˜—è©¦å¸¶ local_onlyï¼›è‹¥èˆŠç‰ˆ class ä¸æ”¯æ´æ­¤åƒæ•¸ï¼Œå°±é€€å›åªçµ¦ memory_path
    try:
        _AGI_AGENT = WuyunAGIAgent(
            memory_path="wuyun_agent_memory.jsonl",
            local_only=local_only,
        )
    except TypeError:
        _AGI_AGENT = WuyunAGIAgent(
            memory_path="wuyun_agent_memory.jsonl",
        )

    return _AGI_AGENT


@app.post("/memory/store")
def memory_store():
    """
    RAG ä¿®è£œ v4ï¼š
    - ä¸å†å‘¼å« WuyunAGIAgentï¼ˆé¿å… rag_base_url / åƒæ•¸ä¸ç›¸å®¹å•é¡Œï¼‰
    - å–®ç´”æŠŠ content è½åœ°æˆæª”æ¡ˆï¼Œä¹‹å¾Œè¦å†åšé€²ä¸€æ­¥ AGI è¨˜æ†¶ï¼Œå¯å¦å¤–å¯«å·¥å…·è™•ç†

    POST /memory/store
    {
      "content": "...",          # å¿…å¡«
      "metadata": { ... }        # å¯é¸ï¼Œæœƒå¯«åœ¨æª”æ¡ˆå‰å…©è¡Œ JSON
    }
    """
    js = request.get_json(force=True, silent=True) or {}
    content = (js.get("content") or "").strip()
    meta = js.get("metadata") or js.get("meta") or {}

    if not content:
        return add_cors(
            jsonify({"ok": False, "error": "empty_content"}),
            request.headers.get("Origin"),
        )

    # ç°¡å–®è½åœ°åˆ° memory_store/ æ–¹ä¾¿ä¹‹å¾Œå†åšé›¢ç·šè™•ç†æˆ–æ‰¹æ¬¡åŒ¯å…¥å‘é‡åº«
    try:
        base_dir = os.path.join(os.path.dirname(__file__), "memory_store")
        os.makedirs(base_dir, exist_ok=True)
        ts = int(time.time())
        fname = os.path.join(base_dir, f"mem_{ts}.txt")
        with open(fname, "w", encoding="utf-8") as f:
            if meta:
                f.write(json.dumps(meta, ensure_ascii=False) + "\n\n")
            f.write(content)
    except Exception as e:
        # å¯«æª”å¤±æ•—ä¹Ÿä¸è¦è®“ API æ›æ‰
        return add_cors(
            jsonify({"ok": False, "error": f"write_failed: {e}"}),
            request.headers.get("Origin"),
        )

    return add_cors(
        jsonify({"ok": True, "stored": content}),
        request.headers.get("Origin"),
    )


# ---- è¨˜æ†¶æœå°‹ v3ï¼ˆä¿®å¾©ç‰ˆï¼‰ ----
@app.route("/memory/search", methods=["POST", "OPTIONS"])
def memory_search_v3():
    """
    RAG ä¿®å¾©ç‰ˆ v3
    åŠŸèƒ½ï¼š
      - æ¥æ”¶ {"query": "...", "top_k": 5}
      - å‘¼å«ç¾æœ‰ search_similar(query, top_k)
      - å›å‚³æ ¼å¼ï¼š
        {
          "ok": true,
          "results": [
            { "text": "...", "score": 0.87, "metadata": {...} }
          ]
        }
    èªªæ˜ï¼š
      - ä¾è³´ç¾æœ‰ search_similar()
      - ä¸å‹• indexã€ä¸æ”¹ corpus æ ¼å¼
      - metadata ç›®å‰å…ˆä¿ç•™ç©º dictï¼Œæœªä¾†è¦åŠ  category / source å†æ“´å……
    """
    if request.method == "OPTIONS":
        return cors_preflight()

    js = request.get_json(force=True, silent=True) or {}
    query = (js.get("query") or "").strip()
    top_k = int(js.get("top_k") or js.get("limit") or 5)

    if not query:
        return add_cors(jsonify({
            "ok": False,
            "error": "empty_query",
            "results": []
        }), request.headers.get("Origin"))

    try:
        hits = rag_search(query, top_k) if top_k > 0 else []
    except Exception as e:
        print("[memory/search] ERROR:", e)
        return add_cors(jsonify({
            "ok": False,
            "error": f"search_failed: {e.__class__.__name__}",
            "results": []
        }), request.headers.get("Origin"))

    results = []
    for h in (hits or []):
        if isinstance(h, dict):
            text = h.get("text") or h.get("chunk") or ""
            score = float(h.get("score") or h.get("similarity") or 0.0)
            meta = h.get("meta") or h.get("metadata") or {}
        else:
            text = str(h)
            score = 0.0
            meta = {}

        results.append({
            "text": text,
            "score": score,
            "metadata": meta,
        })

    return add_cors(jsonify({
        "ok": True,
        "results": results
    }), request.headers.get("Origin"))


@app.route("/agent", methods=["POST", "OPTIONS"])
def agent_entry():
    """
    ç„¡è˜Š AGI ä»£ç†å…¥å£ï¼š
    POST /agent
    {
      "query": "å•é¡Œå…§å®¹",
      "max_tokens": 512   # å¯é¸
    }
    """
    if request.method == "OPTIONS":
        return cors_preflight()

    js = request.get_json(force=True, silent=True) or {}
    query = (js.get("query") or "").strip()
    max_tokens = int(js.get("max_tokens") or 512)

    if not query:
        return add_cors(
            jsonify({"ok": False, "error": "empty_query"}),
            request.headers.get("Origin"),
        )

    # ä½¿ç”¨å”¯ä¸€çš„å…¨åŸŸã€ç„¡è˜Šä¹‹å¿ƒã€
    result = AGI.answer(query, max_tokens=max_tokens)

    return add_cors(
        jsonify({
            "ok": True,
            **result,
        }),
        request.headers.get("Origin"),
    )


# ---- é—œæ–¼èˆ‡è·¯ç”±æ¸…å–® ----
@app.route("/about", methods=["GET", "OPTIONS"])
def about():
    if request.method == "OPTIONS":
        return cors_preflight()
    data = {"name": "RAG Server", "version": "integrated-2025-10+ragfix3"}
    return add_cors(jsonify(data), request.headers.get("Origin"))


@app.route("/routes", methods=["GET", "OPTIONS"])
def routes():
    if request.method == "OPTIONS":
        return cors_preflight()
    lst = sorted([str(r.rule) for r in app.url_map.iter_rules()])
    return add_cors(jsonify({"ok": True, "routes": lst}), request.headers.get("Origin"))


# ---- å¥åº· ----
@app.route("/health", methods=["GET", "OPTIONS"])
def health():
    if request.method == "OPTIONS":
        return cors_preflight()
    data = {
        "dim": EMB_DIM,
        "index_size": 0 if DOC_EMBS is None else int(DOC_EMBS.shape[0]),
        "doc_texts": int(len(DOC_TEXTS)),
        "unique_sources": int(len(set(DOC_SOURCES))),
        "ok": True,
    }
    return add_cors(jsonify(data), request.headers.get("Origin"))


# ---- ç´¢å¼•ä¾†æºæª¢æŸ¥ï¼ˆæ¸¬è©¦ç”¨ï¼‰ ----
@app.route("/sources", methods=["GET", "OPTIONS"])
def list_sources():
    """åˆ—å‡ºå·²é€²ç´¢å¼•çš„ sourcesï¼Œå¯ç”¨ contains= ä¾†éæ¿¾ï¼ˆç”¨æ–¼é©—æ”¶æŸæª”æ˜¯å¦è¢« ingestï¼‰ã€‚"""
    if request.method == "OPTIONS":
        return cors_preflight()
    contains = (request.args.get("contains", "") or "").strip()
    srcs = list(DOC_SOURCES or [])
    if contains:
        srcs = [s for s in srcs if contains in s]
    # é¿å…å›å¤ªå¤§
    limit = int(request.args.get("limit", "200") or "200")
    srcs = srcs[:max(1, min(limit, 2000))]
    return add_cors(jsonify({"ok": True, "count": len(srcs), "sources": srcs}), request.headers.get("Origin"))


# ---- è¨­å®šè®€å¯« ----
@app.route("/config", methods=["GET", "POST", "OPTIONS"])
def config():
    if request.method == "OPTIONS":
        return cors_preflight()
    if request.method == "GET":
        return add_cors(jsonify({**CONFIG, "ok": True}), request.headers.get("Origin"))
    try:
        js = request.get_json(force=True, silent=True) or {}
        for k, v in js.items():
            if k in CONFIG:
                CONFIG[k] = v
        return add_cors(jsonify({**CONFIG, "ok": True}), request.headers.get("Origin"))
    except Exception as e:
        return add_cors(jsonify({"ok": False, "error": str(e)}), request.headers.get("Origin"))


# ---- é‡æ–°è¼‰å…¥ç´¢å¼• ----
@app.route("/reload", methods=["POST", "OPTIONS"])
def reload_index():
    global DOC_EMBS

    if request.method == "OPTIONS":
        return cors_preflight()

    info = rebuild_index()

    add_log({
        "type": "reload",
        "ok": bool(info.get("ok")),
        "docs": int(info.get("doc_texts") or 0),
        "dim": int(info.get("dim") or EMB_DIM),
        "t": float(info.get("t") or 0.0),
        "unique_sources": int(info.get("unique_sources") or 0),
    })

    return add_cors(jsonify({
        "ok": True,
        "dim": int(info.get("dim") or EMB_DIM),
        "doc_texts": int(info.get("doc_texts") or 0),
        "index_size": int(info.get("index_size") or 0),
        "unique_sources": int(info.get("unique_sources") or 0),
        "t": float(info.get("t") or 0.0),
    }), request.headers.get("Origin"))

@app.route("/ingest", methods=["POST", "OPTIONS"])
def ingest():
    """å…¼å®¹æ¸¬è©¦è…³æœ¬ï¼šPOST JSON ä¾‹å¦‚
    {"content":"...","filename":"AIåŸºç¡€çŸ¥è¯†.txt","mode":"append"}
    æˆ– {"text":"..."}ï¼ˆcontent/text æ“‡ä¸€ï¼‰
    """
    if request.method == "OPTIONS":
        return cors_preflight()
    js = request.get_json(force=True, silent=True) or {}
    content = js.get("content") or js.get("text") or ""
    content = str(content)
    if not content.strip():
        return add_cors(jsonify({"ok": False, "error": "empty_content"}), request.headers.get("Origin"))

    root_dir = RAG_ROOT
    knowledge_dir = os.path.join(root_dir, "knowledge")
    os.makedirs(knowledge_dir, exist_ok=True)

    # æª”åæ¸…ç†ï¼ˆé¿å…è·¯å¾‘ç©¿è¶Šï¼‰
    filename = js.get("filename") or js.get("name") or ""
    filename = str(filename).strip()
    if filename:
        filename = os.path.basename(filename)
        if not (filename.lower().endswith(".txt") or filename.lower().endswith(".md")):
            filename = filename + ".txt"
    else:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ingest_{ts}.txt"

    mode = str(js.get("mode") or "append").lower()
    path = os.path.join(knowledge_dir, filename)

    write_mode = "a" if mode == "append" else "w"
    with open(path, write_mode, encoding="utf-8") as f:
        if write_mode == "a":
            f.write("\n\n")
        f.write(content.strip() + "\n")

    # ç«‹åˆ»é‡å»ºç´¢å¼•ï¼Œè®“ä¸‹ä¸€æ¬¡ /search /ask ç«‹åˆ»å¯ç”¨
    rebuild_index()
    return add_cors(jsonify({
        "ok": True,
        "saved_to": os.path.relpath(path, root_dir).replace("\\", "/"),
        "index_size": 0 if DOC_EMBS is None else int(DOC_EMBS.shape[0]),
        "doc_texts": int(len(DOC_TEXTS)),
        "unique_sources": int(len(set(DOC_SOURCES))),
        "dim": EMB_DIM,
    }), request.headers.get("Origin"))


@app.route("/reset", methods=["POST", "OPTIONS"])
def reset():
    """å¯é¸ï¼šæ¸…ç©ºè¨˜æ†¶é«”ç´¢å¼•ï¼ˆä¸åˆªæª”ï¼‰ã€‚"""
    if request.method == "OPTIONS":
        return cors_preflight()
    global DOC_EMBS, DOC_TEXTS, DOC_SOURCES, GRAPH
    DOC_EMBS = np.zeros((0, EMB_DIM), dtype=np.float32)
    DOC_TEXTS, DOC_SOURCES = [], []
    GRAPH = None
    return add_cors(jsonify({"ok": True, "reset": True}), request.headers.get("Origin"))


@app.route("/rag_debug", methods=["POST", "OPTIONS"])
def rag_debug():
    """å›å‚³ raw hits èˆ‡éæ¿¾å¾Œ hitsï¼Œä¾¿æ–¼é©—æ”¶ã€æŠ€è¡“é¡Œåªç”¨ knowledgeã€ã€‚"""
    if request.method == "OPTIONS":
        return cors_preflight()
    js = request.get_json(force=True, silent=True) or {}
    query = (js.get("query") or "").strip()
    top_k = int(js.get("top_k") or 5)
    raw = search_similar(query, max(1, top_k) * 6) if query else []
    filtered = filter_hits_for_query(query, raw) if query else []
    for h in raw:
        h.setdefault("content", h.get("text", ""))
    for h in filtered:
        h.setdefault("content", h.get("text", ""))
    return add_cors(jsonify({
        "ok": True,
        "query": query,
        "tech_mode": is_tech_question(query),
        "raw": raw[: max(1, top_k) * 3],
        "filtered": filtered[: max(1, top_k)],
    }), request.headers.get("Origin"))


# ---- æœå°‹ ----
@app.route("/search", methods=["POST", "OPTIONS"])
def search():
    if request.method == "OPTIONS":
        return cors_preflight()

    payload = request.get_json(silent=True) or {}
    query = payload.get("query") or payload.get("q") or payload.get("text") or ""
    query = str(query).strip()
    top_k = payload.get("top_k", payload.get("k", 5))
    try:
        top_k = int(top_k)
    except Exception:
        top_k = 5
    top_k = max(1, min(50, top_k))

    if not query:
        return add_cors(jsonify({"ok": False, "error": "empty_query", "hits": []}), request.headers.get("Origin"))

    try:
        hits = search_similar(query, top_k=top_k)
        add_log({"type": "search", "q": query[:200], "top_k": top_k, "hits": len(hits), "preview": [h.get("source", "") for h in hits[:3]]})
        return add_cors(jsonify({"ok": True, "query": query, "top_k": top_k, "hits": hits}), request.headers.get("Origin"))
    except Exception as e:
        import traceback
        tb = traceback.format_exc(limit=10)
        add_log({"type": "search_error", "q": query[:200], "error": str(e), "trace": tb})
        return add_cors(jsonify({"ok": False, "error": "search_failed", "message": str(e)}), request.headers.get("Origin"))

@app.route("/infer", methods=["POST", "OPTIONS"])
def infer():
    if request.method == "OPTIONS":
        return cors_preflight()

    payload = request.get_json(silent=True) or {}

    query = payload.get("query") or payload.get("prompt") or payload.get("text") or ""
    if not query and isinstance(payload.get("messages"), list):
        msgs = payload.get("messages") or []
        for msg in reversed(msgs):
            if isinstance(msg, dict) and msg.get("role") == "user":
                query = msg.get("content") or ""
                break

    query = str(query).strip()
    if not query:
        return add_cors(jsonify({"ok": False, "error": "empty_query"}), request.headers.get("Origin"))

    try:
        max_tokens = int(payload.get("max_tokens", CONFIG.get("LOCAL_MAX_TOKENS", 256)))
    except Exception:
        max_tokens = int(CONFIG.get("LOCAL_MAX_TOKENS", 256))

    force_local = bool(payload.get("force_local", False))
    force_cloud = bool(payload.get("force_cloud", False))

    try:
        answer, used = decide_and_infer(query, max_tokens=max_tokens, force_local=force_local, force_cloud=force_cloud)
        return add_cors(jsonify({"ok": True, "used": used, "answer": answer}), request.headers.get("Origin"))
    except Exception as e:
        import traceback
        tb = traceback.format_exc(limit=10)
        add_log({"type": "infer_error", "q": query[:200], "error": str(e), "trace": tb})
        return add_cors(jsonify({"ok": False, "error": "infer_failed", "message": str(e)}), request.headers.get("Origin"))

@app.route("/ask", methods=["POST", "OPTIONS"])
def ask():
    if request.method == "OPTIONS":
        return cors_preflight()
    t0 = time.time()
    js = request.get_json(force=True, silent=True) or {}
    query = (js.get("query") or "").strip()
    if not query:
        return add_cors(jsonify({"ok": False, "error": "empty_query"}), request.headers.get("Origin"))

    top_k = int(js.get("top_k") or 3)
    max_tokens = int(js.get("max_tokens") or CONFIG["LOCAL_MAX_TOKENS"])
    force_local = bool(js.get("force_local"))
    force_cloud = bool(js.get("force_cloud"))
    one_line = bool(js.get("one_line"))

    hits = rag_search(query, top_k) if top_k and top_k > 0 else []
    ref_text = "\n".join([h["text"] for h in hits]) if hits else ""

    tech_mode = is_tech_question(query)

    # çµ„ Promptï¼ˆæŠ€è¡“é¡Œé¿å…èª“æ–‡é¢¨æ ¼ï¼‰
    tech_mode = is_tech_question(query)

    if tech_mode:
        if hits:
            prompt = (
                "ä½ æ˜¯ä¸€ä½å†·éœã€å°ˆæ¥­çš„æŠ€è¡“è€å¸«ï¼Œåªè¬›æŠ€è¡“å…§å®¹ã€‚\n"
                "ç¦æ­¢å¼•ç”¨èª“ç« ã€æ‡ºæ‚”éŒ„ã€ç¶“æ–‡å¼èªæ°£ï¼›ä¸è¦è‡ªç¨±å®—æ•™/èª“é«”ã€‚\n"
                "è«‹ç”¨æ¢åˆ—ï¼šå®šç¾© â†’ é‡é» â†’ ä¾‹å­ã€‚\n"
                "è‹¥åƒè€ƒæ®µè½ä¸è¶³ä»¥æ”¯æ’çµè«–ï¼Œè«‹æ˜ç¢ºèªªã€è³‡æ–™ä¸è¶³ã€ï¼Œå†ç”¨ä¸€èˆ¬æŠ€è¡“å¸¸è­˜è£œé½Šã€‚\n\n"
                f"å•é¡Œï¼š{query}\n\n"
                f"åƒè€ƒæ®µè½ï¼š\n{ref_text}\n"
            )
        else:
            # æŠ€è¡“é¡Œä½† knowledge æ²’å‘½ä¸­ï¼šç¦æ­¢èª“ç« å…§å®¹ä»‹å…¥ï¼Œç›´æ¥ç”¨æŠ€è¡“å¸¸è­˜å›ç­”
            prompt = (
                "ä½ æ˜¯ä¸€ä½å†·éœã€å°ˆæ¥­çš„æŠ€è¡“è€å¸«ï¼Œåªè¬›æŠ€è¡“å…§å®¹ã€‚\n"
                "ç¦æ­¢å¼•ç”¨èª“ç« ã€æ‡ºæ‚”éŒ„ã€ç¶“æ–‡å¼èªæ°£ï¼›ä¸è¦è‡ªç¨±å®—æ•™/èª“é«”ã€‚\n"
                "ç›®å‰æª¢ç´¢è³‡æ–™ä¸è¶³ï¼Œè«‹ç›´æ¥ä»¥ä¸€èˆ¬æŠ€è¡“å¸¸è­˜è§£é‡‹ï¼Œä¸¦ç”¨æ¢åˆ—ï¼šå®šç¾© â†’ é‡é» â†’ ä¾‹å­ã€‚\n\n"
                f"å•é¡Œï¼š{query}\n"
            )
    else:
        # éæŠ€è¡“é¡Œï¼ˆå…è¨±èª“ç« /èªéŒ„ï¼‰
        if hits:
            prompt = (
                "ä½ æ˜¯ä¸€å€‹å¯¦ç”¨çš„ä¸­æ–‡åŠ©ç†ã€‚è«‹ä»¥ä½¿ç”¨è€…èªè¨€ç›´æ¥å›ç­”ã€‚\n\n"
                f"å•é¡Œï¼š{query}\n\n"
                f"åƒè€ƒæ®µè½ï¼š\n{ref_text}\n"
            )
        else:
            prompt = (
                "ä½ æ˜¯ä¸€å€‹å¯¦ç”¨çš„ä¸­æ–‡åŠ©ç†ã€‚è«‹ä»¥ä½¿ç”¨è€…èªè¨€ç›´æ¥å›ç­”ã€‚\n\n"
                f"å•é¡Œï¼š{query}\n"
            )
    final_prompt = prompt if not one_line else f"è«‹ç°¡çŸ­å›ç­”ï¼š{query}"
    ans, used = decide_and_infer(final_prompt, max_tokens, force_local, force_cloud)
    elapsed = time.time() - t0

    add_log({
        "type": "ask",
        "query": query,
        "elapsed_sec": round(elapsed, 3),
        "used": used,
        "top_hit": hits[0] if hits else None,
        "writeback": (used == "cloud"),
        "writeback_reason": "ok" if used == "cloud" else None,
    })

    resp = {
        "ok": True,
        "answer": ans,
        "elapsed_sec": round(elapsed, 3),
        "used": used,
        "hit_threshold": CONFIG["HIT_USE_TH"],
        "min_overlap": CONFIG["MIN_OVERLAP"],
        "top_hit": hits[0] if hits else None,
    }
    return add_cors(jsonify(resp), request.headers.get("Origin"))


# ---- GraphRAG ç‹€æ…‹æŸ¥è©¢ ----
@app.route("/graph/info", methods=["GET", "OPTIONS"])
def graph_info():
    if request.method == "OPTIONS":
        return cors_preflight()
    if GRAPH is None:
        return add_cors(jsonify({"ok": False, "error": "graph_not_built_or_networkx_missing"}), request.headers.get("Origin"))
    return add_cors(
        jsonify({
            "ok": True,
            "nodes": int(GRAPH.number_of_nodes()),
            "edges": int(GRAPH.number_of_edges()),
        }),
        request.headers.get("Origin"),
    )


# ---- Logs ----
@app.route("/logs", methods=["GET", "OPTIONS"])
def logs():
    if request.method == "OPTIONS":
        return cors_preflight()
    limit = int(request.args.get("limit", "20"))
    data = CALL_LOGS[-limit:]
    return add_cors(jsonify({"ok": True, "count": len(data), "logs": data}), request.headers.get("Origin"))


@app.route("/logs/stats", methods=["GET", "OPTIONS"])
def logs_stats():
    if request.method == "OPTIONS":
        return cors_preflight()
    total = len(CALL_LOGS)
    used_count = {"local": 0, "cloud": 0, "local_error": 0}
    writeback_count = {"true": 0, "false": 0}
    for e in CALL_LOGS:
        used = e.get("used")
        if used in used_count:
            used_count[used] += 1
        else:
            used_count["local_error"] += 1
        if e.get("writeback"):
            writeback_count["true"] += 1
        else:
            writeback_count["false"] += 1
    return add_cors(jsonify({
        "ok": True,
        "stats": {
            "since": None,
            "total": total,
            "used": used_count,
            "writeback": writeback_count,
        },
    }), request.headers.get("Origin"))


@app.route("/logs/clear", methods=["POST", "OPTIONS"])
def logs_clear():
    if request.method == "OPTIONS":
        return cors_preflight()
    n = len(CALL_LOGS)
    CALL_LOGS.clear()
    return add_cors(jsonify({"ok": True, "cleared": n}), request.headers.get("Origin"))


# ====== å•Ÿå‹• ======
def startup():
    load_embedder()
    rebuild_index()
    log.info("Routes: /about, /agent, /ask, /config, /graph/info, /health, /infer, /logs, /logs/clear, /logs/stats, /memory/store, /memory/search, /oath/list, /oath/save, /oath/generate, /reload, /routes, /search, /static/<path:filename>")
    log.info("ğŸš€ RAG server starting at http://0.0.0.0:7000")
    log.info("RAG_ROOT=%s", RAG_ROOT)
    log.info("Embedding dim=%s, index_size=%s", EMB_DIM, 0 if DOC_EMBS is None else int(DOC_EMBS.shape[0]))
    log.info("FORCE_MODE=%s, LOCAL_LLM_URL=%s", CONFIG["FORCE_MODE"], CONFIG["LOCAL_LLM_URL"])


if __name__ == "__main__":
    startup()
    app.run(host="0.0.0.0", port=7000, debug=False)

# =====================
# AGI Agent å®‰å…¨æ•´åˆè£œä¸ï¼ˆä¸åˆªåŸç¢¼ï¼‰
# =====================
@app.route("/agent", methods=["POST"])
def agent():
    """
    ç„¡è˜Š AGI å¾Œç«¯ä»‹é¢ï¼š
    Chatbot-UI / å…¶ä»–å‰ç«¯åªè¦ POST JSON:
        {"query": "...", "max_tokens": 256}
    å°±æœƒç”±ç„¡è˜Š v5.1 å›ç­”ã€‚
    """
    data = request.get_json(force=True) or {}
    query = data.get("query", "").strip()
    max_tokens = int(data.get("max_tokens", 512))

    



def build_system_prompt(question: str) -> str:
    tech_keywords = ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†", "AI", "ç®—æ³•", "æ¨¡å‹"]
    if any(k in question for k in tech_keywords):
        return (
            "ä½ æ˜¯ä¸€ä½å†·éœã€å°ˆæ¥­çš„æŠ€è¡“è€å¸«ï¼Œ"
            "ç”¨ç°¡å–®ä¸­æ–‡èªªæ˜æ¦‚å¿µï¼Œæ¢åˆ—é‡é»ï¼Œ"
            "ä¸è¦åŠ å…¥å®—æ•™ã€èª“é¡˜ã€ç¶“æ–‡é¢¨æ ¼ï¼Œåªè¬›æŠ€è¡“å…§å®¹ã€‚"
        )
    return DEFAULT_WUYUN_SYSTEM_PROMPT
